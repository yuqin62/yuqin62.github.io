<!DOCTYPE html>
<html>
<head>
    <!--<title>Project 4</title>-->
    <style>
        body {
            padding: 30px;
            width: 1000px;
            margin: auto;
            text-align: left;
            font-weight: 300;
            font-family: Arial, sans-serif;
        }
        h1, h2, h3 {
          font-family: 'Source Sans Pro', sans-serif;
        }
        h3 {
            font-size: 18px;
            font-weight: bold;
        }
        p {
            font-family: 'Source Sans Pro', sans-serif;
            text-align: center;
            font-size: 15px;
        }
        .center {
            text-align: center;
        }
        .image-row, .iframe-row {
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .iframe-row iframe {
            margin: 10px;
        }
        .image-small {
            margin: 20px;
            width: 150px;
            height: auto;
        }
        .image-medium {
            margin: 5px;
            width: 240px;
            height: auto;
        }
        .image-medium-3 {
        margin: 5px;
        width: 240px;
        height: auto;
        }
        .image-medium-4 {
        margin: 3px;
        width: 165px;
        height: auto;
        }
        .image-medium-5 {
        margin: 1.5px;
        width: 130px;
        height: auto;
        }
        .image-medium-7 {
        margin: 1px;
        width: 100px;
        height: auto;
        }
        .image-large {
            margin: 10px;
            width: 600px;
            height: auto;
        }
        figcaption {
            font-family: 'Source Sans Pro', sans-serif;
            font-size: 12px;
            font-weight: normal;
            text-align: center;
            margin-top: 5px;
        }    
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1 align="middle">Project 5: Fun With Diffusion Models</h1>
    <br>
    <h2 align="middle">Yuqin Jiao</h2>
    <h2 align="middle">Part A: The Power of Diffusion Models</h2>
    <h2 align="middle">Overview for Part A</h2>
    <p>
        Part A is for using pre-trained UNet, implementing diffusion sampling loops, and using them for other tasks such as inpainting and creating optical illusions.
    </p>
    <h2 align="middle">Part 0: Setup</h2>
    <p>
        For Part 0, I set the random seed to 180 to ensure consistent results across runs. Using the provided prompts ‚Äî "an oil painting of a snowy mountain village," 
        "a man wearing a hat," and "a rocket ship" ‚Äî I sampled images from both stage 1 (64x64 resolution) and stage 2 (256x256 resolution). For each stage, I experimented with 
        num_inference_steps set to 5, 20, and 40. The outputs demonstrated varying levels of detail and alignment with the prompts across inference steps, to be specific, 
        with the increase of num_inference_steps value, the image are higher quality. Also, generally, the stage 2 output images are higher quality than the stage 1
        output images.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/51.png" alt="Description of figure1_1">
            <figcaption>[num_inference_steps as 5] stage 1 output: an oil painting of a snowy mountain village</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/52.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 5] stage 1 output: a man wearing a hat</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/53.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 5] stage 1 output: a rocket ship</figcaption>
        </figure>
    </div>
    <br>
    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/54.png" alt="Description of figure1_1">
            <figcaption>[num_inference_steps as 5] stage 2 output: an oil painting of a snowy mountain village</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/55.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 5] stage 2 output: a man wearing a hat</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/56.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 5] stage 2 output: a rocket ship</figcaption>
        </figure>
    </div>
    <br>
    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/201.png" alt="Description of figure1_1">
            <figcaption>[num_inference_steps as 20] stage 1 output: an oil painting of a snowy mountain village</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/202.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 20] stage 1 output: a man wearing a hat</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/203.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 20] stage 1 output: a rocket ship</figcaption>
        </figure>
    </div>
    <br>
    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/204.png" alt="Description of figure1_1">
            <figcaption>[num_inference_steps as 20] stage 2 output: an oil painting of a snowy mountain village</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/205.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 20] stage 2 output: a man wearing a hat</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/206.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 20] stage 2 output: a rocket ship</figcaption>
        </figure>
    </div>
    <br>
    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/401.png" alt="Description of figure1_1">
            <figcaption>[num_inference_steps as 40] stage 1 output: an oil painting of a snowy mountain village</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/402.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 40] stage 1 output: a man wearing a hat</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/403.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 40] stage 1 output: a rocket ship</figcaption>
        </figure>
    </div>
    <br>
    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/404.png" alt="Description of figure1_1">
            <figcaption>[num_inference_steps as 40] stage 2 output: an oil painting of a snowy mountain village</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/405.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 40] stage 2 output: a man wearing a hat</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/406.png" alt="Description of figure1_2">
            <figcaption>[num_inference_steps as 40] stage 2 output: a rocket ship</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1: Sampling Loops</h2>
    <p>
        In part 1, I wrote my own "sampling loops" that use the pretrained DeepFloyd denoisers.
    </p>
    <h2 align="middle">Part 1.1: Implementing the Forward Process</h2>
    <p></p>
        I implemented the forward(im, t) function, which generates a noisy image x_t by adding scaled Gaussian noise œµ to a clean 
        image x_0. The noise is generated using torch.randn_like(). I tested the function with the provided Campanile image at noise 
        levels t=[250,500,750], progressively increasing the noise. The scaling factors sqrt(Œ±_bar_t) and sqrt(1 - Œ±_bar_t) were 
        used to adjust the image and noise. The outputs showed a clear increase in noise as t increased.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-4" src="media/Berkeley Campanile.png" alt="Description of figure1_1">
            <figcaption>[test image] Berkeley Campanile</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=250.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=250</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=500.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=500</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=750.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=750</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.2: Classical Denoising</h2>
    <p>
        I applied Gaussian blur filtering to remove noise from the noisy images at t=[250,500,750]. Using torchvision.transforms.functional.gaussian_blur, I 
        experimented with kernel sizes and sigma values to produce denoised images. While the Gaussian blur reduced high-frequency noise, the results demonstrated 
        the limitations of classical filtering, as it was unable to fully recover the details of the original image.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=250.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=250</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=500.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=500</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=750.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=750</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-medium-4" src="media/Gaussian Blur Denoising at t=250.png" alt="Description of figure1_1">
            <figcaption>[Classical Denoising] Gaussian Blur Denoising at t=250</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Gaussian Blur Denoising at t=500.png" alt="Description of figure1_2">
            <figcaption>[Classical Denoising] Gaussian Blur Denoising at t=500</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Gaussian Blur Denoising at t=750.png" alt="Description of figure1_2">
            <figcaption>[Classical Denoising] Gaussian Blur Denoising at t=750</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.3: One-Step Denoising</h2>
    <p>
        I utilized the pretrained diffusion model (stage_1.unet) to denoise the noisy images. The UNet predicted the noise œµ, which I subtracted from the noisy 
        image x_t to estimate the original clean image x_0. I adhered to the diffusion equation x_0 = (x_t - sqrt(Œ±_bar_t)*œµ)/sqrt(Œ±_bar_t). The results for t=[250,500,750] 
        demonstrated a significant improvement in recovering the image, highlighting the superiority of the diffusion model over classical methods. The text prompt 
        "a high quality photo" was used for conditioning the UNet during denoising.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-4" src="media/Berkeley Campanile.png" alt="Description of figure1_1">
            <figcaption>[original image]] Berkeley Campanile</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=250.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=250</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=500.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=500</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Noisy Campanile at t=750.png" alt="Description of figure1_2">
            <figcaption>[test image at noise level] Noisy Campanile at t=750</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-medium-4" src="media/Gaussian Blur Denoising at t=250.png" alt="Description of figure1_1">
            <figcaption>[One Step Denoising] Denoised estimate at t=250</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Gaussian Blur Denoising at t=500.png" alt="Description of figure1_2">
            <figcaption>[One Step Denoising] Denoised estimate at t=500</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/Gaussian Blur Denoising at t=750.png" alt="Description of figure1_2">
            <figcaption>[One Step Denoising] Denoised estimate at t=750</figcaption>
        </figure>
    </div>
    
    <h2 align="middle">Part 1.4: Iterative Denoising</h2>
    <p>
        I implemented iterative denoising using the pretrained diffusion model (stage_1.unet). The process starts with a 
        highly noisy image at timestep t, and noise is progressively removed step by step using a custom schedule (strided_timesteps). 
        At each step, the model estimates the noise and reduces it, gradually recovering the original image. The text prompt is set as "a high quality photo." 
        To optimize the process, I skipped timesteps using a stride of 30, reducing computational cost while maintaining quality. The results were compared 
        to one-step denoising and Gaussian blur. Iterative denoising produced significantly cleaner images, demonstrating the model's ability to effectively handle high noise levels.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/noisy campanile t=90.png" alt="Description of figure1_1">
            <figcaption>Noisy Campanile at t=90</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/noisy campanile t=240.png" alt="Description of figure1_2">
            <figcaption>Noisy Campanile at t=240</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/noisy campanile t=390.png" alt="Description of figure1_2">
            <figcaption>Noisy Campanile at t=390</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/noisy campanile t=540.png" alt="Description of figure1_2">
            <figcaption>Noisy Campanile at t=540</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/noisy campanile t=690.png" alt="Description of figure1_2">
            <figcaption>Noisy Campanile at t=690</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-medium-4" src="media/Original.png" alt="Description of figure1_1">
            <figcaption>Original</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/iteratively denoised campanile.png" alt="Description of figure1_2">
            <figcaption>Iteratively Denoised Campanile</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/one-step denoised campanile.png" alt="Description of figure1_2">
            <figcaption>One-Step Denoised Campanile</figcaption>
        </figure>
        <figure>
            <img class="image-medium-4" src="media/gaussian blurred campanile.png" alt="Description of figure1_2">
            <figcaption>Gaussian Blurred Campanile</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.5: Diffusion Model Sampling</h2>
    <p>
        I used the pretrained diffusion model (stage_1.unet) to generate images from random noise. By setting i_start = 0 in the 
        iterative_denoise function, the model denoised pure noise step by step. Five images were generated using the text prompt 
        "a high quality photo" and displayed as results.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/sample 1.png" alt="Description of figure1_1">
            <figcaption>Sample 1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 2.png" alt="Description of figure1_2">
            <figcaption>Sample 2</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 3.png" alt="Description of figure1_2">
            <figcaption>Sample 3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 4.png" alt="Description of figure1_2">
            <figcaption>Sample 4</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 5.png" alt="Description of figure1_2">
            <figcaption>Sample 5</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.6: Classifier-Free Guidance (CFG)</h2>
    <p>
        I implemented Classifier-Free Guidance (CFG) using the diffusion model (stage_1.unet) to enhance image 
        quality by combining conditional and unconditional noise estimates. With a CFG scale of 7, I generated five 
        high-quality images from random noise, showcasing improved results compared to standard sampling techniques.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/sample 1 with cfg.png" alt="Description of figure1_1">
            <figcaption>Sample 1 with CFG</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 2 with cfg.png" alt="Description of figure1_2">
            <figcaption>Sample 2 with CFG</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 3 with cfg.png" alt="Description of figure1_2">
            <figcaption>Sample 3 with CFG</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 4 with cfg.png" alt="Description of figure1_2">
            <figcaption>Sample 4 with CFG</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/sample 5 with cfg.png" alt="Description of figure1_2">
            <figcaption>Sample 5 with CFG</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.7: Image-to-image Translation</h2>
    <p>
        I implemented image-to-image translation using the pretrained diffusion model (stage_1.unet) and 
        Classifier-Free Guidance (CFG). Starting with a slightly noisy image, the model iteratively denoised 
        it to create "edits" that gradually resemble the original image. Using noise levels [1,3,5,7,10,20], 
        I generated edits with the prompt "a high-quality photo."
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/SDEdit00 i_start=1.png" alt="Description of figure1_1">
            <figcaption>SDEdit with i_start=1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit00 i_start=3.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit00 i_start=5.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit00 i_start=7.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit00 i_start=10.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit00 i_start=20.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/test00.png" alt="Description of figure1_2">
            <figcaption>Campanile</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/SDEdit01 i_start=1.png" alt="Description of figure1_1">
            <figcaption>SDEdit with i_start=1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit01 i_start=3.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit01 i_start=5.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit01 i_start=7.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit01 i_start=10.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit01 i_start=20.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/test01.png" alt="Description of figure1_2">
            <figcaption>Cube</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/SDEdit02 i_start=1.png" alt="Description of figure1_1">
            <figcaption>SDEdit with i_start=1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit02 i_start=3.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit02 i_start=5.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit02 i_start=7.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit02 i_start=10.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/SDEdit02 i_start=20.png" alt="Description of figure1_2">
            <figcaption>SDEdit with i_start=20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/test02.png" alt="Description of figure1_2">
            <figcaption>Playground</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.7.1: Editing Hand-Drawn and Web Images</h2>
    <p>
        I used the pretrained diffusion model (stage_1.unet) with Classifier-Free Guidance (CFG) to transform non-realistic 
        images, such as sketches and web images, into natural-looking images. Using the interaction tool provided, I edited 
        one web image and two hand-drawn images by applying iterative denoising at noise levels [1,3,5,7,10,20]. The results 
        demonstrate the model's ability to project creative edits onto the natural image manifold.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/scene00 i_start=1.png" alt="Description of figure1_1">
            <figcaption>[web image] Scene at i_start=1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/scene00 i_start=3.png" alt="Description of figure1_2">
            <figcaption>[web image] Scene at i_start=3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/scene00 i_start=5.png" alt="Description of figure1_2">
            <figcaption>[web image] Scene at i_start=5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/scene00 i_start=7.png" alt="Description of figure1_2">
            <figcaption>[web image] Scene at i_start=7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/scene00 i_start=10.png" alt="Description of figure1_2">
            <figcaption>[web image] Scene at i_start=10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/scene00 i_start=20.png" alt="Description of figure1_2">
            <figcaption>[web image] Scene at i_start=20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/scene00.png" alt="Description of figure1_2">
            <figcaption>[web image] Scene</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/draw00 i_start=1.png" alt="Description of figure1_1">
            <figcaption>[drawn image] Pavilion at i_start=1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/Sdraw00 i_start=3.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Pavilion at i_start=3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw00 i_start=5.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Pavilion at i_start=5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw00 i_start=7.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Pavilion at i_start=7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw00 i_start=10.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Pavilion at i_start=10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw00 i_start=20.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Pavilion at i_start=20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw00.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Pavilion</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/draw01 i_start=1.png" alt="Description of figure1_1">
            <figcaption>[drawn image] Triangle at i_start=1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw01 i_start=3.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Triangle at i_start=3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw01 i_start=5.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Triangle at i_start=5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw01 i_start=7.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Triangle at i_start=7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw01 i_start=10.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Triangle at i_start=10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw01 i_start=20.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Triangle at i_start=20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/draw01.png" alt="Description of figure1_2">
            <figcaption>[drawn image] Triangle</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.7.2: Inpainting</h2>
    <p>
        I implemented inpainting using the pretrained diffusion model (stage_1.unet) with Classifier-Free Guidance (CFG), 
        following the RePaint paper. Using a binary mask, I replaced parts of an image with new content while preserving the 
        unmasked regions. The process iteratively added noise and denoised the image while respecting the mask constraints. 
        The figures include the test image inpainted (with a given mask) and two additional images edited using corresponding masks.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/inpaint00.png" alt="Description of figure1_1">
            <figcaption>Campanile</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/mask00.png" alt="Description of figure1_2">
            <figcaption>Mask</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/hole to fill00.png" alt="Description of figure1_2">
            <figcaption>Hole to Fill</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/inpainted00.png" alt="Description of figure1_2">
            <figcaption>Campanile Inpainted</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/inpaint01.png" alt="Description of figure1_1">
            <figcaption>Cube</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/mask01.png" alt="Description of figure1_2">
            <figcaption>Mask</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/hole to fill01.png" alt="Description of figure1_2">
            <figcaption>Hole to Fill</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/inpainted01.png" alt="Description of figure1_2">
            <figcaption>Cube Inpainted</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-5" src="media/inpaint02.png" alt="Description of figure1_1">
            <figcaption>Playground</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/mask02.png" alt="Description of figure1_2">
            <figcaption>Mask</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/hole to fill02.png" alt="Description of figure1_2">
            <figcaption>Hole to Fill</figcaption>
        </figure>
        <figure>
            <img class="image-medium-5" src="media/inpainted02.png" alt="Description of figure1_2">
            <figcaption>Playground Inpainted</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.7.3: Text-Conditional Image-to-image Translation</h2>
    <p>
        I implemented text-conditional image-to-image translation using the pretrained diffusion model (stage_1.unet) with Classifier-Free 
        Guidance (CFG). By combining iterative denoising and text prompts, the model transforms noisy images to align with the text 
        description while retaining features of the original image. The process was applied at noise levels [1,3,5,7,10,20]. The figures with different prompts: 
        1. Rocket Ship: Using the prompt "a rocket ship," the test image was guided to incorporate rocket-like campanile. 2. Pencil: Using the 
        prompt "a pencil," the test image was translated to resemble a pencil-like cube. 3. Hipster Barista: Using the prompt "a photo of a 
        hipster barista," the test image was edited to blend features of the original playground with the text prompt.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/rocket ship at noise level 1.png" alt="Description of figure1_1">
            <figcaption>[rocket ship] rocket ship at noise level 1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/rocket ship at noise level 3.png" alt="Description of figure1_2">
            <figcaption>[rocket ship] rocket ship at noise level 3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/rocket ship at noise level 5.png" alt="Description of figure1_2">
            <figcaption>[rocket ship] rocket ship at noise level 5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/rocket ship at noise level 7.png" alt="Description of figure1_2">
            <figcaption>[rocket ship] rocket ship at noise level 7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/rocket ship at noise level 10.png" alt="Description of figure1_2">
            <figcaption>[rocket ship] rocket ship at noise level 10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/rocket ship at noise level 20.png" alt="Description of figure1_2">
            <figcaption>[rocket ship] rocket ship at noise level 20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/test00.png" alt="Description of figure1_2">
            <figcaption>[rocket ship] Campanile</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/pencil at noise level 1.png" alt="Description of figure1_1">
            <figcaption>[pencil] pencil at noise level 1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/pencil at noise level 3.png" alt="Description of figure1_2">
            <figcaption>[pencil] pencil at noise level 3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/pencil at noise level 5.png" alt="Description of figure1_2">
            <figcaption>[pencil] pencil at noise level 5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/pencil at noise level 7.png" alt="Description of figure1_2">
            <figcaption>[pencil] pencil at noise level 7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/pencil at noise level 10.png" alt="Description of figure1_2">
            <figcaption>[pencil] pencil at noise level 10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/pencil at noise level 20.png" alt="Description of figure1_2">
            <figcaption>[pencil] pencil at noise level 20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/test01.png" alt="Description of figure1_2">
            <figcaption>[pencil] Cube</figcaption>
        </figure>
    </div>

    <div class="image-row">
        <figure>
            <img class="image-medium-7" src="media/barista at noise level 1.png" alt="Description of figure1_1">
            <figcaption>[hipster barista] barista at noise level 1</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/barista at noise level 3.png" alt="Description of figure1_2">
            <figcaption>[hipster barista] barista at noise level 3</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/barista at noise level 5.png" alt="Description of figure1_2">
            <figcaption>[hipster barista] barista at noise level 5</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/barista at noise level 7.png" alt="Description of figure1_2">
            <figcaption>[hipster barista] barista at noise level 7</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/barista at noise level 10.png" alt="Description of figure1_2">
            <figcaption>[hipster barista] barista at noise level 10</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/barista at noise level 20.png" alt="Description of figure1_2">
            <figcaption>[hipster barista] barista at noise level 20</figcaption>
        </figure>
        <figure>
            <img class="image-medium-7" src="media/test02.png" alt="Description of figure1_2">
            <figcaption>[hipster barista] Playground</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.8: Visual Anagrams</h2>
    <p>
        I implemented Visual Anagrams using the pretrained diffusion model (stage_1.unet) with Classifier-Free Guidance (CFG). 
        The first group of outputs combines two noise estimates guided by different prompts: "an oil painting of an old man" and "an oil painting of people around a campfire". 
        The second group of outputs combines two noise estimates guided by different prompts: "a photo of a hipster barista" and "an oil painting of an old man". 
        The third group of outputs combines two noise estimates guided by different prompts: "a pencil" and "a man wearing a hat". 
        By flipping the image and corresponding noise estimate, averaging the results, and denoising iteratively, the model generates an image that appears differently when flipped upside down.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/people around a campfire1.png" alt="Description of figure1_1">
            <figcaption>[Prompt] An Oil Painting of an Old Man</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/people around a campfire2.png" alt="Description of figure1_2">
            <figcaption>[Prompt] An Oil Painting of People around a Campfire</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/hipster barista1.png" alt="Description of figure1_1">
            <figcaption>[Prompt] A Photo of A Hipster Barista</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/hipster barista2.png" alt="Description of figure1_2">
            <figcaption>[Prompt] An Oil Painting of an Old Man</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/a pencil1.png" alt="Description of figure1_1">
            <figcaption>[Prompt] A Pencil</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/a pencil2.png" alt="Description of figure1_2">
            <figcaption>[Prompt] A Man Wearing A Hat</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.9: Hybrid Images</h2>
    <p>
        I implemented Hybrid Images using the pretrained diffusion model (stage_1.unet) with Classifier-Free Guidance (CFG). 
        By combining noise estimates from two prompts‚Äî"a lithograph of a skull" and "a lithograph of waterfalls"‚Äîthe model 
        creates a composite noise estimate. Low frequencies from one noise estimate are blended with high frequencies from the 
        other using a Gaussian blur (kernel size 33, sigma 2). The final hybrid image appears as a skull from afar and transforms 
        into waterfalls when viewed up close. I also tried another two groups of prompts, which are "a pencil" with "a rocket ship" 
        and "an oil painting of people around a campfire" with "a photo of a dog".
    </p>
    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/download (3).png" alt="Description of figure1_1">
            <figcaption>Hybrid image of a skull and a waterfall</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/download (2).png" alt="Description of figure1_2">
            <figcaption>Hybrid image of a rocket ship and a pencil</figcaption>
        </figure>
        <figure>
            <img class="image-medium-3" src="media/download (1).png" alt="Description of figure1_2">
            <figcaption>Hybrid image of an oil painting of people around a campfire and a dog</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part B: Diffusion Models from Scratch</h2>
    <h2 align="middle">Overview for Part B</h2>
    <p>
        Part B is for implementing diffusion model and training on MNIST dataset.
    </p>
    <h2 align="middle">Part 1: Training a Single-Step Denoising UNet</h2>
    <h2 align="middle">Part 1.1: Implementing the UNet</h2>
    <p>
        I implemented a UNet-based architecture as the denoiser for the project. 
        According to the instruction and images from website, the UNet features downsampling and upsampling blocks, 
        enabling detailed reconstruction of noisy images. Key components include convolutional layers (Conv), 
        pooling layers (Flatten/ Unflatten), and concatenation operations (Concat). The model uses BatchNorm for normalization 
        and GELU as the activation function, with the hidden dimension set to 128 for the diffusion model's network.
    </p>
    <h2 align="middle">Part 1.2: Using the UNet to Train a Denoiser</h2>
    <p>
        In this part1.2, the goal was to train the UNet to map noisy images z=x+œÉœµ to clean images x. The training data consisted of 
        clean MNIST images and their corresponding noisy versions generated with noise levels œÉ=0.5. The model was optimized with an L2 loss to 
        minimize the reconstruction error. Noisy images were dynamically generated during training to enhance generalization.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part1 noisy image.png" alt="Description of figure1_2">
            <figcaption>Varying levels of noise on MNIST digits</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.2.1: Training</h2>
    <p>
        The UNet was trained on the MNIST dataset for 5 epochs using a batch size of 256 and the Adam optimizer (learning rate 10^(‚àí4)). Loss curves were 
        tracked during training, and denoised outputs were visualized after the 1st and 5th epochs. Results demonstrated clear improvement in the model's ability to 
        recover clean images, highlighting effective learning of denoising tasks.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part1 training loss.png" alt="Description of figure1_2">
            <figcaption>Training Loss Curve</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part1 epoch1.png" alt="Description of figure1_2">
            <figcaption>Results on digits from the test set after 1 epoch of training</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part1 epoch5.png" alt="Description of figure1_2">
            <figcaption>Results on digits from the test set after 5 epochs of training</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 1.2.2: Out-of-Distribution Testing</h2>
    <p>
        To evaluate robustness, the trained UNet was tested on MNIST images noised with varying œÉ values ([0.0,0.2,0.4,0.5,0.6,0.8,1.0]). The denoiser performed well for 
        noise levels close to the training distribution (œÉ=0.5), with gradually reduced performance as noise deviated further. This testing highlighted the 
        model's limitations and its adaptability to unseen noise levels.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part1 same image1.png" alt="Description of figure1_2">
            <figcaption> Results on digits from the test set with varying noise levels</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part1 same image2.png" alt="Description of figure1_2">
            <figcaption> Results on digits from the test set with varying noise levels</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 2: Training a Diffusion Model</h2>
    <h2 align="middle">Part 2.1: Adding Time Conditioning to UNet</h2>
    <p>
        In this part, I extended the UNet architecture from Part 1 by incorporating time-conditioning to predict noise at various timesteps t. This involved 
        normalizing the scalar t to [0, 1] and embedding it into the network using FCBlock layers. These embeddings modulated key components like unflatten and 
        upconv layers, enabling the UNet to adapt its outputs based on t. This adjustment allowed the model to handle noise with varying variance over time, 
        making it suitable for iterative denoising. The rest of the UNet structure remained unchanged from Part 1.
    </p>
    <h2 align="middle">Part 2.2: Training the UNet</h2>
    <p>
        Using the modified UNet, I trained the model on the MNIST dataset with random noise added dynamically across 300 timesteps t. For each training step, I 
        generated noisy images x_t using the DDPM schedule and optimized the UNet to predict the noise œµ. The Adam optimizer with an exponentially decaying 
        learning rate ensured stable convergence across 20 epochs. By conditioning the UNet on t, the model learned to denoise images iteratively, effectively reversing the 
        noising process. The time-conditioned UNet demonstrated robust performance in noise prediction, producing clean results from noisy inputs.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part2.2 training loss curve.png" alt="Description of figure1_2">
            <figcaption>Time-Conditioned UNet training loss curve</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 2.3: Sampling from the UNet</h2>
    <p>
        In part2.3, I implemented the DDPM sampling process based on Algorithm B.2 to generate images from noise using the time-conditioned UNet. The process starts with a pure noise 
        tensor x_T and iteratively refines it across T=300 timesteps using predicted noise œµ_Œ∏. By leveraging precomputed schedules for Œ≤, Œ±, and Œ±_bar, I combined low-variance predictions 
        and stochastic noise at each step to compute x_{t-1}. The implementation allows for intermediate visualizations and outputs final clean images. Results were compared after 5 and 20 
        training epochs, showcasing model progression.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part2.3 epoch5.png" alt="Description of figure1_2">
            <figcaption>[Time Conditioned] Epoch 5</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part2.3 epoch20.png" alt="Description of figure1_2">
            <figcaption>[Time Conditioned] Epoch 20</figcaption>
        </figure>
    </div>

    <h2 align="middle">Part 2.4: Adding Class-Conditioning to UNet</h2>
    <p>
        In this part, I extended the UNet to include class-conditioning by introducing two additional FCBlock layers for the class vector c, which is encoded as a one-hot vector. 
        During training, c is occasionally dropped (10% of the time) for unconditional generation. The model predicts noise ùúñ_ùúÉ(x_t, t, c), conditioned on both time t and class c. Loss 
        computation aligns with Algorithm B.3, which incorporates dropout masking for c. The training loss curve demonstrated progressive improvement across epochs.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part2.4 training loss curve.png" alt="Description of figure1_2">
            <figcaption>Class-conditioned UNet training loss curve</figcaption>
        </figure>
    </div>
    <h2 align="middle">Part 2.5: Sampling from the Class-Conditioned UNet</h2>
    <p>
        In part2.5, the class-conditioned UNet was used to generate images based on specific digit classes (0-9) using classifier-free guidance. To enhance the quality of class-conditional results, a guidance 
        scale Œ≥=5.0 was applied. The model predicted the noise œµ_Œ∏(x_t,t,c), where t is the timestep and c is the class label. Sampling began with random noise (x_T‚àºN(0,I)), iteratively denoising it through T=300 steps 
        using the guidance mechanism. Both unconditional (c=0) and conditional (c‚â†0) noise estimates were computed, and the final prediction combined these with the guidance scale. The output was clamped to [0, 1] for 
        realistic results. Sampling visualizations included grids of digits, with 4 samples per class, after 5 and 20 epochs, showcasing improved generation quality over training.
    </p>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part2.5 epoch=5.png" alt="Description of figure1_2">
            <figcaption>[Class Conditioned] Epoch 5</figcaption>
        </figure>
    </div>
    <div class="image-row">
        <figure>
            <img class="image-large" src="media/part2.5 epoch=20.png" alt="Description of figure1_2">
            <figcaption>[Class Conditioned] Epoch 20</figcaption>
        </figure>
    </div>

    <h2 align="middle">Reflection</h2>
    <p>
        In this project, I learned how to use and train a diffusion model, specifically how to add time conditioning and class conditioning to UNet.
    </p>

    <h2 align="middle">Bells and Whistles 1</h2>
    <p>
        For part 2 of Bells and Whistles in PartA: Create something cool with what you learned in this project, I made a hybrid image of a man sitting with a dog around a campfire.
    </p>

    <div class="image-row">
        <figure>
            <img class="image-medium-3" src="media/download (4).png" alt="Description of figure1_2">
            <figcaption>[Hybrid Image] A Man Sitting with A Dog around A Campfire</figcaption>
        </figure>
    </div>
    
</body>
</html>

